{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f918b54",
   "metadata": {},
   "source": [
    "# 1. Look up the Adam optimization functions in PyTorch https://pytorch.org/docs/stable/optim.html . How does it work? Try at least one other optimization function with the diabetes dataset shown in class. How does the model perform with the new optimizer? Did it perform better or worse than Adam? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ab605",
   "metadata": {},
   "source": [
    "The Adam(Adaptive Movement Estimation) optimizer adapts a learning rate for each input variable for the objective function and further smooths the search process by using an exponentially decreasing moving average of the gradient to make updates to variables.\n",
    "\n",
    "https://machinelearningmastery.com/adam-optimization-from-scratch/\n",
    "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b421c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "diabetes_df = pd.read_csv(\"../week_13/diabetes.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed77fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1).values\n",
    "y = diabetes_df['Outcome'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)\n",
    "\n",
    "# #Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a174f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.93138344  2.0179454   0.78066953 ...  0.43148259 -0.37477883\n",
      "   0.63212912]\n",
      " [ 0.63260632 -1.14861888  0.46538785 ... -0.1198324  -0.29416766\n",
      "   0.71699246]\n",
      " [-0.56250219 -0.47692343 -0.2702694  ... -0.20958135  2.74517192\n",
      "   0.03808578]\n",
      " ...\n",
      " [-0.86127931 -0.76479291  0.04501228 ...  0.76483585 -0.78380586\n",
      "  -0.30136756]\n",
      " [ 0.63260632  2.20985838  1.2010451  ...  0.43148259 -0.60466993\n",
      "   2.75371249]\n",
      " [ 0.03505207  0.73852549 -0.58555107 ... -0.33779414 -0.57779954\n",
      "   0.29267578]]\n",
      "[1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1\n",
      " 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1\n",
      " 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
      " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "#prints a tensor; multidimensional array\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92ed91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch neural network model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #where the activation functions are\n",
    "\n",
    "#create tensors (from our data) which are matrices \n",
    "\n",
    "#redefine X_train and X_test based on the datatypes we need to use top optimize using torch\n",
    "X_train = torch.FloatTensor(X_train) \n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "#y is an integer. May run into issues when working with integers, so use LongTensor\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e8ed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9314,  2.0179,  0.7807,  ...,  0.4315, -0.3748,  0.6321],\n",
      "        [ 0.6326, -1.1486,  0.4654,  ..., -0.1198, -0.2942,  0.7170],\n",
      "        [-0.5625, -0.4769, -0.2703,  ..., -0.2096,  2.7452,  0.0381],\n",
      "        ...,\n",
      "        [-0.8613, -0.7648,  0.0450,  ...,  0.7648, -0.7838, -0.3014],\n",
      "        [ 0.6326,  2.2099,  1.2010,  ...,  0.4315, -0.6047,  2.7537],\n",
      "        [ 0.0351,  0.7385, -0.5856,  ..., -0.3378, -0.5778,  0.2927]])\n"
     ]
    }
   ],
   "source": [
    "#print values to see what's happening to data.\n",
    "#it's been converted to a slightly different format so it's optimized for usage\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a371b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an artificial neural network using classes\n",
    "#it will accept a neural network Model\n",
    "class ANN_Model(nn.Module):\n",
    "    #initialize the class and the attributes created from the class\n",
    "    #various data will be passed in from the nn.Model\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=20, out_features=2):\n",
    "        super().__init__()\n",
    "        #layer 1 connection is everything between the input value and first hidden layer\n",
    "        #it's all of the different connections it's making\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    " \n",
    "    #now build in forward propogation into the class \n",
    "\n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        #x is being redefined each time\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be7ad14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#create instance of model that was created above\n",
    "ann = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "413e23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer: ways to improve the model on top of loss function\n",
    "#lr is learning rate: how fast is it going to learn\n",
    "optimizer = torch.optim.Adamax(ann.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50fbfaee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 0.647470235824585\n",
      "Epoch number: 11 with loss: 0.5535582304000854\n",
      "Epoch number: 21 with loss: 0.4916539788246155\n",
      "Epoch number: 31 with loss: 0.45121264457702637\n",
      "Epoch number: 41 with loss: 0.4320707619190216\n",
      "Epoch number: 51 with loss: 0.4135775864124298\n",
      "Epoch number: 61 with loss: 0.3985571563243866\n",
      "Epoch number: 71 with loss: 0.3826017379760742\n",
      "Epoch number: 81 with loss: 0.36624693870544434\n",
      "Epoch number: 91 with loss: 0.35049572587013245\n",
      "Epoch number: 101 with loss: 0.3354971706867218\n",
      "Epoch number: 111 with loss: 0.3211274743080139\n",
      "Epoch number: 121 with loss: 0.3066723644733429\n",
      "Epoch number: 131 with loss: 0.29085466265678406\n",
      "Epoch number: 141 with loss: 0.2746669054031372\n",
      "Epoch number: 151 with loss: 0.25882044434547424\n",
      "Epoch number: 161 with loss: 0.24144339561462402\n",
      "Epoch number: 171 with loss: 0.2244419902563095\n",
      "Epoch number: 181 with loss: 0.20857742428779602\n",
      "Epoch number: 191 with loss: 0.1923830360174179\n",
      "Epoch number: 201 with loss: 0.1772410124540329\n",
      "Epoch number: 211 with loss: 0.16301529109477997\n",
      "Epoch number: 221 with loss: 0.1484755277633667\n",
      "Epoch number: 231 with loss: 0.1362002193927765\n",
      "Epoch number: 241 with loss: 0.12553516030311584\n",
      "Epoch number: 251 with loss: 0.11654482781887054\n",
      "Epoch number: 261 with loss: 0.108462855219841\n",
      "Epoch number: 271 with loss: 0.10104005038738251\n",
      "Epoch number: 281 with loss: 0.0942678228020668\n",
      "Epoch number: 291 with loss: 0.08778765052556992\n",
      "Epoch number: 301 with loss: 0.08196382224559784\n",
      "Epoch number: 311 with loss: 0.07623257488012314\n",
      "Epoch number: 321 with loss: 0.07114661484956741\n",
      "Epoch number: 331 with loss: 0.06608188897371292\n",
      "Epoch number: 341 with loss: 0.06134972348809242\n",
      "Epoch number: 351 with loss: 0.05696597322821617\n",
      "Epoch number: 361 with loss: 0.05339202657341957\n",
      "Epoch number: 371 with loss: 0.04983516409993172\n",
      "Epoch number: 381 with loss: 0.04677816480398178\n",
      "Epoch number: 391 with loss: 0.04371151328086853\n",
      "Epoch number: 401 with loss: 0.04107217863202095\n",
      "Epoch number: 411 with loss: 0.03868015483021736\n",
      "Epoch number: 421 with loss: 0.03621486574411392\n",
      "Epoch number: 431 with loss: 0.03393072634935379\n",
      "Epoch number: 441 with loss: 0.031799837946891785\n",
      "Epoch number: 451 with loss: 0.02982386387884617\n",
      "Epoch number: 461 with loss: 0.02798604592680931\n",
      "Epoch number: 471 with loss: 0.02629430592060089\n",
      "Epoch number: 481 with loss: 0.02471909485757351\n",
      "Epoch number: 491 with loss: 0.02334698662161827\n"
     ]
    }
   ],
   "source": [
    "#run model\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = ann.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss}')\n",
    "        \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() \n",
    "    optimizer.step() #perform one optimization step each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d9358b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad(): #decreases memory consumption\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = ann(data)\n",
    "        y_pred.append(prediction.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a247e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.76       150\n",
      "           1       0.57      0.62      0.59        81\n",
      "\n",
      "    accuracy                           0.70       231\n",
      "   macro avg       0.68      0.68      0.68       231\n",
      "weighted avg       0.71      0.70      0.70       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db40c6",
   "metadata": {},
   "source": [
    "For the model above, I used Adamax to replace Adam as the optimizer and the performance of the model improved in predicting people who do have diabetes. Yet, the overall performance of predicting those who do have diabetes is still low. \n",
    "Adamax accelerates the optimization process; it adapts a seperate learning rate for each parameter. Adamax uses the infinity norm and is less susceptible to gradient noise.\n",
    "\n",
    "\n",
    "Links for me to review:\n",
    "https://machinelearningknowledge.ai/pytorch-optimizers-complete-guide-for-beginner/\n",
    "https://machinelearningmastery.com/gradient-descent-optimization-with-adamax-from-scratch/\n",
    "https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c#:~:text=The%20learning%20rate%20of%20AdaGrad,a%20saddle%20point%20much%20better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f037521",
   "metadata": {},
   "source": [
    "# 2. Write a function that lists and counts the number of divisors for an input value.\n",
    "Example 1:\n",
    "\n",
    "Input: 5\n",
    "\n",
    "Output: “There are 2 divisors: 1 and 5”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb4ef62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 divisors: [1, 2, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "def print_divisors(x):\n",
    "    #print(\"The divisors of\",x,\"are:\")\n",
    "    count = 0\n",
    "    div = []\n",
    "    for i in range(1, x + 1):\n",
    "        #print(i)\n",
    "        if x % i == 0:\n",
    "            count = count + 1\n",
    "            div.append(i)\n",
    "            \n",
    "#make sure return is outside of for loop\n",
    "    print('There are', count, 'divisors:', div) \n",
    "print_divisors(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7313a",
   "metadata": {},
   "source": [
    "# 2a. Example 2:\n",
    "# Input: 40\n",
    "# Output: “There are 8 divisors: 1, 2, 4, 5, 8, 10, 20, and 40”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ff83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 divisors: [1, 2, 4, 5, 8, 10, 20, 40]\n"
     ]
    }
   ],
   "source": [
    "def print_divisors(x):\n",
    "    #print(\"The divisors of\",x,\"are:\")\n",
    "    count = 0\n",
    "    div = []\n",
    "    for i in range(1, x + 1):\n",
    "        #print(i)\n",
    "        if x % i == 0:\n",
    "            count = count + 1\n",
    "            div.append(i)\n",
    "            \n",
    "#make sure return is outside of for loop\n",
    "    print('There are', count, 'divisors:', div)\n",
    "print_divisors(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236319c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
