{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89807778",
   "metadata": {},
   "source": [
    "Data preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset\n",
    "\n",
    "Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c35e3",
   "metadata": {},
   "source": [
    "## Working with data types\n",
    "\n",
    "One of the steps to think about when preparing data for  modeling is the types that are present in your dataset, because you'll likely have to transform some of these columns to other types later on.\n",
    "\n",
    "Why are types important?\n",
    " - can check the type of a df by using the dtypes attribute: df.dtypes\n",
    "    - 1. object: string/mixed types\n",
    "    - 2. int64: integer\n",
    "    - 3. float64: float\n",
    "- datetime64(or timedelta): datetime\n",
    "\n",
    "### Converting Column types\n",
    "- use the astype() method: df[\"column\"] = df[\"column\"].astype(\"float)\n",
    "print("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d8281",
   "metadata": {},
   "source": [
    "## Strafefied Sampling\n",
    "\n",
    "Train a model on a sample of data that is representative of the entire  dataset\n",
    "\n",
    "## Standardization\n",
    "\n",
    "Standardization is a preprocessing method used to transform continuous data to make it look normally distributed. In scikit-learn, this is often a necessary step, because many models assume that the data you are training on is normally distributed, and if it isn't, you risk biasing your model.\n",
    "- preprocessing method applied to continuous, numerical data. \n",
    "\n",
    "Two methods to standardize your data\n",
    "    1. log normalization\n",
    "    2. scaling\n",
    "    \n",
    "When to standardize: models\n",
    "- if you're working with any kind of model that uses a linear distance metric or operates in a linear space like k-nearest neighbors, linear regression, or k-means clustering, the model is assuming that the data and features you're giving it are related in a linear fashion, or can be measured with a linear distance metric\n",
    "- dataset features have high variance: The case when a feature or features in your dataset have high variance is related to this. This could bias a model that assumes the data is normally distributed.\n",
    "- dataset  features are continuous and on different scales: For example, consider a dataset that contains a column related to height and another related to weight. In order to compare these features, they must be in the same linear space, and therefore must be standardized in some way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23e990",
   "metadata": {},
   "source": [
    "## Log normalization (standardization method)\n",
    "- useful to use when you have a particular column with high variance\n",
    "- Log normalization applies a log transformation to your values, which transforms your values onto a scale that approximates normality, an assumption about your data that a lot of models make\n",
    "    - method: it takes the natural log of each number in the left hand column, which is simply the exponent you would raise above the mathematical constant e (approximately equal to 2.718) to get that number.\n",
    "    - Applying log normalization to data in Python is fairly straightforward. We can use the log function from Numpy to do the trick: \n",
    "    \n",
    "    check the variance of the columns: df.var()\n",
    "    take the log normalization of the column with high variance\n",
    "    -  df['column'] = np.log(df['column'])\n",
    "    now check the variance of all columns to make sure they're closer together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94815600",
   "metadata": {},
   "source": [
    "## Scaling  data (standardization method)\n",
    "Scaling is a method of standardization that's most useful when you're working with a dataset that contains continuous features that are on different scales, and you're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors).\n",
    "\n",
    "- Feature scaling transforms the features in your dataset so they have a mean of zero and a variance of one. \n",
    "- Ex:  In each column, we have numbers that are relatively close within the column, but not across columns. If we look at the variance, it's relatively low across columns. To better model this data, scaling would be a good choice here.\n",
    "    - use sklearn.preprocessing import StandardScaler\n",
    "     scaler = StandardScaler()\n",
    "     df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02224529",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "1. Feature selection\n",
    "Once you've settled on a feature set for modeling, it's important to really consider these features. Do you need all of them, and do you know how they will impact your model?\n",
    "\n",
    "2. What is feature selection?\n",
    "Feature selection is a method of selecting features from your feature set to be used for modeling. It draws from a set of existing features, so it's different than feature engineering because it doesn't create new features. \n",
    "\n",
    "The overarching goal of feature selection is to improve your model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9fc98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
